{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPuKg/90G3MpLwrbaIP/guV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U_t0XMYQac2M","executionInfo":{"status":"ok","timestamp":1764257214157,"user_tz":-420,"elapsed":4799,"user":{"displayName":"Hồ Quang Chung","userId":"12751878129593085762"}},"outputId":"a8cac5d2-74ff-4442-92f2-1ae84c9b647e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sgfmill\n","  Downloading sgfmill-1.1.1-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Downloading sgfmill-1.1.1-py3-none-any.whl (27 kB)\n","Installing collected packages: sgfmill\n","Successfully installed sgfmill-1.1.1\n"]}],"source":["!pip install sgfmill numpy"]},{"cell_type":"code","source":["import numpy as np\n","from sgfmill import sgf, boards\n","\n","# Kích thước bàn cờ\n","BOARD_SIZE = 19\n","# Số lượng history planes (8 cho ta + 8 cho địch = 16)\n","HISTORY_LENGTH = 8\n","\n","def make_input_planes(game_state, color, history_states):\n","    \"\"\"\n","    Tạo ra tensor 17x19x19\n","    color: 'b' hoặc 'w' (người đang đi nước này)\n","    history_states: List các trạng thái bàn cờ trước đó\n","    \"\"\"\n","    features = np.zeros((17, BOARD_SIZE, BOARD_SIZE), dtype=np.float32)\n","\n","    current_player_color = color\n","    opponent_color = 'w' if color == 'b' else 'b'\n","\n","    # --- 16 Kênh Lịch sử (8 Ta + 8 Địch) ---\n","    # Lấy tối đa 8 trạng thái gần nhất, nếu không đủ thì padding số 0\n","    recent_states = history_states[-HISTORY_LENGTH:]\n","    # Đảo ngược để trạng thái mới nhất nằm đầu\n","    recent_states = recent_states[::-1]\n","\n","    for i, state in enumerate(recent_states):\n","        # Kênh quân Ta (0, 2, 4...)\n","        features[i] = (state == current_player_color).astype(np.float32)\n","        # Kênh quân Địch (8, 9, 10...) -> Logic trong AlphaGo Zero xếp xen kẽ hoặc tách khối\n","        # Ở đây ta xếp: 8 kênh Ta trước, 8 kênh Địch sau cho dễ hình dung\n","        features[i + 8] = (state == opponent_color).astype(np.float32)\n","\n","    # --- Kênh thứ 17: Màu quân (Ai đi lượt này?) ---\n","    # Nếu Đen đi: toàn số 1. Nếu Trắng đi: toàn số 0.\n","    if color == 'b':\n","        features[16] = np.ones((BOARD_SIZE, BOARD_SIZE), dtype=np.float32)\n","\n","    return features\n","\n","def parse_move(move_coords, board_size=19):\n","    \"\"\"Chuyển tọa độ (row, col) thành số nguyên 0-360. Pass là 361\"\"\"\n","    if move_coords is None:\n","        return board_size * board_size # Nước Pass\n","    row, col = move_coords\n","    return row * board_size + col"],"metadata":{"id":"QU1CQJ2kahla","executionInfo":{"status":"ok","timestamp":1764257214177,"user_tz":-420,"elapsed":7,"user":{"displayName":"Hồ Quang Chung","userId":"12751878129593085762"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58ae13f1","executionInfo":{"status":"ok","timestamp":1764257224481,"user_tz":-420,"elapsed":10304,"user":{"displayName":"Hồ Quang Chung","userId":"12751878129593085762"}},"outputId":"1b00537c-71fe-4495-ef1b-887b5655b194"},"source":["get_ipython().system('sudo apt-get update')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n","\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Connecting to security.\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n","Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,153 kB]\n","Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,835 kB]\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,008 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n","Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,488 kB]\n","Get:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n","Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,290 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,596 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,876 kB]\n","Get:21 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,539 kB]\n","Fetched 37.5 MB in 6s (6,546 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e07127e3","executionInfo":{"status":"ok","timestamp":1764257229453,"user_tz":-420,"elapsed":4971,"user":{"displayName":"Hồ Quang Chung","userId":"12751878129593085762"}},"outputId":"3b5bdff7-9280-4392-9723-06c9bbfac38e"},"source":["get_ipython().system('sudo apt-get install -y p7zip-full')"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","p7zip-full is already the newest version (16.02+dfsg-8).\n","0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n"]}]},{"cell_type":"code","metadata":{"id":"e25472de","executionInfo":{"status":"ok","timestamp":1764257229573,"user_tz":-420,"elapsed":113,"user":{"displayName":"Hồ Quang Chung","userId":"12751878129593085762"}}},"source":["get_ipython().system('mkdir -p Datasets')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"817f0427","executionInfo":{"status":"ok","timestamp":1764257242031,"user_tz":-420,"elapsed":12451,"user":{"displayName":"Hồ Quang Chung","userId":"12751878129593085762"}},"outputId":"bd4254c8-9d1b-4349-ca54-fc67d1292df3"},"source":["import glob\n","\n","# Find all .7z files in the current directory\n","seven_zip_files = glob.glob('*.7z')\n","\n","# Extract each .7z file into the Datasets directory\n","for file in seven_zip_files:\n","    print(f\"Extracting {file} to Datasets/\")\n","    get_ipython().system(f'7z x {file} -oDatasets/')\n","print(\"Extraction complete.\")"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting Pro.7z to Datasets/\n","\n","7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n","p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n","\n","Scanning the drive for archives:\n","  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 3657149 bytes (3572 KiB)\n","\n","Extracting archive: Pro.7z\n","--\n","Path = Pro.7z\n","Type = 7z\n","Physical Size = 3657149\n","Headers Size = 137267\n","Method = LZMA2:16\n","Solid = +\n","Blocks = 1\n","\n","  0%\b\b\b\b    \b\b\b\b  4% 468 - Pro/3p/1380549114019999712.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% 877 - Pro/3p/1512834770010001313.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% 1553 - Pro/6p/1476180003019999348.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 2153 - Pro/7p/1436793400019999187.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 2431 - Pro/7p/1461079022019999461.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 2582 - Pro/7p/1471877408019999467.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% 2823 - Pro/8p/1483022274019999225.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 3286 - Pro/9p/1479125592019999878.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% 3548 - Pro/9p/1500029087019999511.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 3767 - Pro/9p/1500684758019999332.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 3928 - Pro/9p/1501232678019999670.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 4061 - Pro/9p/1501577110019999702.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 4179 - Pro/9p/1501925495019999195.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 4276 - Pro/9p/1502249105019999298.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 4428 - Pro/9p/1502870115019999466.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 4514 - Pro/9p/1503142346019999554.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 4596 - Pro/9p/1503384408019999694.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 4675 - Pro/9p/1503759511019999018.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 4735 - Pro/9p/1503996391019999326.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 4825 - Pro/9p/1504233921019999677.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 4885 - Pro/9p/1504365726019999321.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 4956 - Pro/9p/1504630827019999837.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 4996 - Pro/9p/1504772908019999088.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 5034 - Pro/9p/1504866441019999370.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 5073 - Pro/9p/1504965129019999735.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 5129 - Pro/9p/1505129767019999051.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 5190 - Pro/9p/1505283475019999563.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 5262 - Pro/9p/1505489562019999277.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% 5378 - Pro/9p/1505916159019999042.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 5428 - Pro/9p/1506062259019999012.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 5510 - Pro/9p/1506234471019999626.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 5578 - Pro/9p/1506414284010001126.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 5730 - Pro/9p/1507164570010001918.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 5953 - Pro/9p/1507706352010001687.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 6155 - Pro/9p/1508332868010001435.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 6362 - Pro/9p/1508761894010001743.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 6549 - Pro/9p/1509350694010001295.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 6717 - Pro/9p/1509830317010001437.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 6880 - Pro/9p/1510155806010001042.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 7043 - Pro/9p/1510494324010001344.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 7210 - Pro/9p/1510999062010001952.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 7369 - Pro/9p/1511338760010001099.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 7526 - Pro/9p/1511723631010001936.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 7677 - Pro/9p/1512097104010001607.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% 7812 - Pro/9p/1512394333010001210.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% 7954 - Pro/9p/1512790307010001164.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 8091 - Pro/9p/1513179321010001967.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 8224 - Pro/9p/1513496420010001601.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 8355 - Pro/9p/1513938610010001572.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 8477 - Pro/9p/1514270359010001561.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 8603 - Pro/9p/1514601689010001117.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 8724 - Pro/9p/1515006648010001561.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 8840 - Pro/9p/1515297411010001170.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 8942 - Pro/9p/1515517247010001632.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 9057 - Pro/9p/1515827632010001587.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 9142 - Pro/9p/1516067532010001151.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 9254 - Pro/9p/1516358958010001049.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 9365 - Pro/9p/1516712167010001589.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 9473 - Pro/9p/1517020796010001012.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 9575 - Pro/9p/1517265417010001845.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 9675 - Pro/9p/1517496402010001751.sgf\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n","\n","Folders: 10\n","Files: 9669\n","Size:       14707514\n","Compressed: 3657149\n","Extraction complete.\n"]}]},{"cell_type":"code","source":["import os\n","import glob\n","import numpy as np\n","from sgfmill import sgf, boards\n","\n","# --- CẤU HÌNH ---\n","DATA_ROOT = './Datasets/Pro/'      # Thư mục gốc chứa 1p, 2p...\n","OUTPUT_DIR = './Datasets/processed/' # Nơi lưu file .npy\n","CHUNK_SIZE = 2000             # Cứ 2000 ván thì lưu ra 1 file (để tránh tràn RAM)\n","BOARD_SIZE = 19\n","HISTORY_LENGTH = 8            # 8 nước lịch sử\n","\n","# Tạo thư mục output nếu chưa có\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)\n","\n","# --- HÀM HỖ TRỢ (GIỮ NGUYÊN) ---\n","def make_input_planes(game_state, color, history_states):\n","    features = np.zeros((17, BOARD_SIZE, BOARD_SIZE), dtype=np.float32)\n","    current_player_color = color\n","    opponent_color = 'w' if color == 'b' else 'b'\n","\n","    # Lấy lịch sử, đảo ngược để mới nhất lên đầu\n","    recent_states = history_states[-HISTORY_LENGTH:][::-1]\n","\n","    for i, state in enumerate(recent_states):\n","        # 8 kênh Ta\n","        features[i] = (state == current_player_color).astype(np.float32)\n","        # 8 kênh Địch\n","        features[i + 8] = (state == opponent_color).astype(np.float32)\n","\n","    # Kênh 17: Màu quân đi\n","    if color == 'b':\n","        features[16] = np.ones((BOARD_SIZE, BOARD_SIZE), dtype=np.float32)\n","\n","    return features\n","\n","def parse_move(move_coords):\n","    if move_coords is None:\n","        return BOARD_SIZE * BOARD_SIZE # Pass\n","    row, col = move_coords\n","    return row * BOARD_SIZE + col\n","\n","def save_chunk(features, policies, values, chunk_id):\n","    \"\"\"Lưu dữ liệu tạm ra file\"\"\"\n","    print(f\"--> Saving chunk {chunk_id} with {len(features)} moves...\")\n","    np.save(os.path.join(OUTPUT_DIR, f\"features_{chunk_id}.npy\"), np.array(features, dtype=np.float32))\n","    np.save(os.path.join(OUTPUT_DIR, f\"labels_policy_{chunk_id}.npy\"), np.array(policies, dtype=np.int64))\n","    np.save(os.path.join(OUTPUT_DIR, f\"labels_value_{chunk_id}.npy\"), np.array(values, dtype=np.float32))\n","\n","# --- HÀM XỬ LÝ CHÍNH ---\n","def process_all_folders():\n","    # Danh sách folder cần quét (từ 1p đến 9p)\n","    # Nếu tên folder của bạn khác (vd: '1p_games'), hãy sửa lại list này\n","    target_folders = [f\"{i}p\" for i in range(1, 10)]\n","\n","    # Bộ đệm tạm thời\n","    buffer_features = []\n","    buffer_policies = []\n","    buffer_values = []\n","\n","    chunk_counter = 0\n","    total_games_processed = 0\n","\n","    # Duyệt qua từng folder 1p, 2p...\n","    for folder_name in target_folders:\n","        folder_path = os.path.join(DATA_ROOT, folder_name)\n","\n","        # Tìm tất cả file .sgf trong folder này\n","        sgf_files = glob.glob(os.path.join(folder_path, \"*.sgf\"))\n","        print(f\"Processing folder: {folder_name} - Found {len(sgf_files)} files\")\n","\n","        for file_path in sgf_files:\n","            try:\n","                with open(file_path, \"rb\") as f:\n","                    content = f.read()\n","\n","                # SGFmill parse\n","                try:\n","                    game = sgf.Sgf_game.from_bytes(content)\n","                except ValueError:\n","                    continue # Bỏ qua file lỗi\n","\n","                # Lọc dữ liệu: Chỉ lấy 19x19 và không chấp quân\n","                if game.get_size() != 19 or (game.get_handicap() is not None and game.get_handicap() > 0):\n","                    continue\n","\n","                winner = game.get_winner()\n","                if winner is None: continue\n","\n","                # Replay ván đấu\n","                board = boards.Board(19)\n","                current_numpy_board = np.zeros((19, 19), dtype=object)\n","                history_boards = [current_numpy_board.copy()]\n","\n","                for node in game.get_main_sequence():\n","                    color, move_coords = node.get_move()\n","                    if color is None: continue\n","\n","                    # 1. Tạo Feature\n","                    input_tensor = make_input_planes(current_numpy_board, color, history_boards)\n","\n","                    # 2. Tạo Label\n","                    policy_target = parse_move(move_coords)\n","                    value_target = 1.0 if winner == color else -1.0\n","\n","                    # Thêm vào buffer\n","                    buffer_features.append(input_tensor)\n","                    buffer_policies.append(policy_target)\n","                    buffer_values.append(value_target)\n","\n","                    # 3. Apply move cho vòng lặp sau\n","                    if move_coords is not None:\n","                        row, col = move_coords\n","                        board.play(row, col, color)\n","                        # Sync sang numpy\n","                        new_numpy_board = np.zeros((19, 19), dtype=object)\n","                        for r in range(19):\n","                            for c in range(19):\n","                                p = board.get(r, c)\n","                                if p: new_numpy_board[r, c] = p\n","                        current_numpy_board = new_numpy_board\n","\n","                    history_boards.append(current_numpy_board.copy())\n","\n","                total_games_processed += 1\n","\n","                # CƠ CHẾ CHUNKING: Kiểm tra nếu buffer đầy thì lưu\n","                # Ở đây ta check theo số ván (game), hoặc số moves\n","                # Nếu buffer đạt khoảng 100.000 mẫu (moves) thì lưu là vừa đẹp\n","                if len(buffer_features) >= 50000: # Khoảng 200-300 ván\n","                    save_chunk(buffer_features, buffer_policies, buffer_values, chunk_counter)\n","                    chunk_counter += 1\n","                    # Reset buffer để giải phóng RAM\n","                    buffer_features = []\n","                    buffer_policies = []\n","                    buffer_values = []\n","                    print(f\"Cleared RAM. Total games so far: {total_games_processed}\")\n","\n","            except Exception as e:\n","                print(f\"Skipping file {file_path}: {e}\")\n","\n","    # Lưu nốt phần còn lại trong buffer (nếu có)\n","    if len(buffer_features) > 0:\n","        save_chunk(buffer_features, buffer_policies, buffer_values, chunk_counter)\n","\n","    print(f\"=== COMPLETED ===\")\n","    print(f\"Total games processed: {total_games_processed}\")\n","    print(f\"Data saved to: {OUTPUT_DIR}\")\n","\n","if __name__ == \"__main__\":\n","    process_all_folders()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_zLp5gDqcFkL","executionInfo":{"status":"ok","timestamp":1764257708707,"user_tz":-420,"elapsed":466668,"user":{"displayName":"Hồ Quang Chung","userId":"12751878129593085762"}},"outputId":"b5282a88-c5ff-4bd4-9b3a-ba766608ae71"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing folder: 1p - Found 293 files\n","--> Saving chunk 0 with 50053 moves...\n","Cleared RAM. Total games so far: 266\n","Processing folder: 2p - Found 151 files\n","Skipping file ./Datasets/Pro/2p/1384432811019999870.sgf: \n","Processing folder: 3p - Found 446 files\n","Skipping file ./Datasets/Pro/3p/1440209016019999364.sgf: \n","Skipping file ./Datasets/Pro/3p/1503025471019999928.sgf: \n","Skipping file ./Datasets/Pro/3p/1489043325019999769.sgf: \n","Skipping file ./Datasets/Pro/3p/1487833529019999156.sgf: \n","Skipping file ./Datasets/Pro/3p/1490862288019999897.sgf: \n","Skipping file ./Datasets/Pro/3p/1491466159019999355.sgf: \n","--> Saving chunk 1 with 50177 moves...\n","Cleared RAM. Total games so far: 515\n","Skipping file ./Datasets/Pro/3p/1500612367019999290.sgf: \n","Skipping file ./Datasets/Pro/3p/1482114863019999178.sgf: \n","Skipping file ./Datasets/Pro/3p/1500519692019999167.sgf: \n","Skipping file ./Datasets/Pro/3p/1491985248019999753.sgf: \n","Skipping file ./Datasets/Pro/3p/1489648230019999778.sgf: \n","Skipping file ./Datasets/Pro/3p/1482997173019999279.sgf: \n","Skipping file ./Datasets/Pro/3p/1441251931019999518.sgf: \n","Skipping file ./Datasets/Pro/3p/1439819361019999462.sgf: \n","Skipping file ./Datasets/Pro/3p/1490324760019999121.sgf: \n","Skipping file ./Datasets/Pro/3p/1488438066019999876.sgf: \n","Skipping file ./Datasets/Pro/3p/1440427304019999648.sgf: \n","Skipping file ./Datasets/Pro/3p/1439608752019999521.sgf: \n","Skipping file ./Datasets/Pro/3p/1440641280019999977.sgf: \n","Skipping file ./Datasets/Pro/3p/1437406668019999606.sgf: \n","Skipping file ./Datasets/Pro/3p/1502074355019999978.sgf: \n","Skipping file ./Datasets/Pro/3p/1388816331019999246.sgf: \n","Processing folder: 4p - Found 292 files\n","Skipping file ./Datasets/Pro/4p/1513751797010001440.sgf: \n","Skipping file ./Datasets/Pro/4p/1513754930010001780.sgf: \n","Skipping file ./Datasets/Pro/4p/1494233966019999998.sgf: \n","Skipping file ./Datasets/Pro/4p/1513579794010001257.sgf: \n","Skipping file ./Datasets/Pro/4p/1499655389019999671.sgf: \n","Skipping file ./Datasets/Pro/4p/1499929079019999035.sgf: \n","--> Saving chunk 2 with 50226 moves...\n","Cleared RAM. Total games so far: 762\n","Skipping file ./Datasets/Pro/4p/1494840076019999593.sgf: \n","Skipping file ./Datasets/Pro/4p/1513666201010001886.sgf: \n","Skipping file ./Datasets/Pro/4p/1496304082019999694.sgf: \n","Skipping file ./Datasets/Pro/4p/1499829675019999050.sgf: \n","Skipping file ./Datasets/Pro/4p/1508482168010001670.sgf: \n","Skipping file ./Datasets/Pro/4p/1513596582010001078.sgf: \n","Skipping file ./Datasets/Pro/4p/1492949752019999218.sgf: \n","Skipping file ./Datasets/Pro/4p/1497864632019999341.sgf: \n","Skipping file ./Datasets/Pro/4p/1494160824019999401.sgf: \n","Skipping file ./Datasets/Pro/4p/1509967696010001996.sgf: \n","Skipping file ./Datasets/Pro/4p/1505723975019999792.sgf: \n","Skipping file ./Datasets/Pro/4p/1492343506019999419.sgf: \n","Skipping file ./Datasets/Pro/4p/1495445077019999477.sgf: \n","Skipping file ./Datasets/Pro/4p/1508157554010001419.sgf: \n","Processing folder: 5p - Found 241 files\n","--> Saving chunk 3 with 50091 moves...\n","Cleared RAM. Total games so far: 1015\n","Skipping file ./Datasets/Pro/5p/1424747211019999142.sgf: \n","Skipping file ./Datasets/Pro/5p/1424525405019999058.sgf: \n","Skipping file ./Datasets/Pro/5p/1424661667019999118.sgf: \n","Processing folder: 6p - Found 372 files\n","Skipping file ./Datasets/Pro/6p/1384352797019999019.sgf: \n","--> Saving chunk 4 with 50071 moves...\n","Cleared RAM. Total games so far: 1261\n","Skipping file ./Datasets/Pro/6p/1384446162019999481.sgf: \n","--> Saving chunk 5 with 50086 moves...\n","Cleared RAM. Total games so far: 1517\n","Skipping file ./Datasets/Pro/6p/1389183603019999816.sgf: \n","Processing folder: 7p - Found 942 files\n","Skipping file ./Datasets/Pro/7p/1454051203019999946.sgf: \n","Skipping file ./Datasets/Pro/7p/1433561074019999479.sgf: \n","--> Saving chunk 6 with 50049 moves...\n","Cleared RAM. Total games so far: 1771\n","Skipping file ./Datasets/Pro/7p/1433061864019999410.sgf: \n","Skipping file ./Datasets/Pro/7p/1433559314019999343.sgf: \n","Skipping file ./Datasets/Pro/7p/1434290609019999400.sgf: \n","--> Saving chunk 7 with 50144 moves...\n","Cleared RAM. Total games so far: 2027\n","--> Saving chunk 8 with 50014 moves...\n","Cleared RAM. Total games so far: 2287\n","Skipping file ./Datasets/Pro/7p/1425700418019999463.sgf: \n","Processing folder: 8p - Found 373 files\n","--> Saving chunk 9 with 50000 moves...\n","Cleared RAM. Total games so far: 2543\n","--> Saving chunk 10 with 50041 moves...\n","Cleared RAM. Total games so far: 2794\n","Processing folder: 9p - Found 6559 files\n","--> Saving chunk 11 with 50141 moves...\n","Cleared RAM. Total games so far: 3030\n","--> Saving chunk 12 with 50290 moves...\n","Cleared RAM. Total games so far: 3269\n","Skipping file ./Datasets/Pro/9p/1379606328019999393.sgf: \n","--> Saving chunk 13 with 50068 moves...\n","Cleared RAM. Total games so far: 3505\n","--> Saving chunk 14 with 11087 moves...\n","=== COMPLETED ===\n","Total games processed: 3557\n","Data saved to: ./Datasets/processed/\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, num_filters=128):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(num_filters)\n","        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(num_filters)\n","\n","    def forward(self, x):\n","        residual = x\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += residual\n","        out = F.relu(out)\n","        return out\n","\n","class GoResNet(nn.Module):\n","    def __init__(self, num_blocks=10, num_filters=128):\n","        super(GoResNet, self).__init__()\n","        self.conv_input = nn.Sequential(\n","            nn.Conv2d(17, num_filters, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(num_filters),\n","            nn.ReLU()\n","        )\n","        self.res_blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n","\n","        # Policy Head\n","        self.policy_head = nn.Sequential(\n","            nn.Conv2d(num_filters, 2, kernel_size=1, bias=False),\n","            nn.BatchNorm2d(2),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(2 * 19 * 19, 362)\n","        )\n","\n","        # Value Head\n","        self.value_head = nn.Sequential(\n","            nn.Conv2d(num_filters, 1, kernel_size=1, bias=False),\n","            nn.BatchNorm2d(1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(19 * 19, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 1),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv_input(x)\n","        for block in self.res_blocks:\n","            x = block(x)\n","        policy = self.policy_head(x)\n","        value = self.value_head(x)\n","        return policy, value"],"metadata":{"id":"UXsyCZ3me6iB","executionInfo":{"status":"ok","timestamp":1764257708708,"user_tz":-420,"elapsed":17,"user":{"displayName":"Hồ Quang Chung","userId":"12751878129593085762"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset, ConcatDataset\n","import numpy as np\n","import glob\n","import os\n","\n","class GoChunkDataset(Dataset):\n","    def __init__(self, feature_path, policy_path, value_path):\n","        # mmap_mode='r' cực kỳ quan trọng trên Colab để không tràn RAM\n","        self.features = np.load(feature_path, mmap_mode='r')\n","        self.policies = np.load(policy_path, mmap_mode='r')\n","        self.values = np.load(value_path, mmap_mode='r')\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        # .copy() để chuyển data từ ổ cứng vào RAM\n","        return {\n","            'feature': torch.tensor(self.features[idx].copy(), dtype=torch.float32),\n","            'policy_target': torch.tensor(self.policies[idx].copy(), dtype=torch.long),\n","            'value_target': torch.tensor(self.values[idx].copy(), dtype=torch.float32)\n","        }\n","\n","def get_dataset(data_dir):\n","    # Lưu ý: data_dir trên Colab sẽ là '/content/dataset'\n","    feature_files = sorted(glob.glob(os.path.join(data_dir, \"features_*.npy\")))\n","    datasets = []\n","    print(f\"Scanning chunks in {data_dir}...\")\n","\n","    for f_path in feature_files:\n","        try:\n","            filename = os.path.basename(f_path)\n","            # Giả sử format tên file là features_{id}.npy\n","            chunk_id = filename.split('_')[1].split('.')[0]\n","\n","            p_path = os.path.join(data_dir, f\"labels_policy_{chunk_id}.npy\")\n","            v_path = os.path.join(data_dir, f\"labels_value_{chunk_id}.npy\")\n","\n","            if os.path.exists(p_path) and os.path.exists(v_path):\n","                ds = GoChunkDataset(f_path, p_path, v_path)\n","                datasets.append(ds)\n","        except Exception as e:\n","            print(f\"Error loading chunk {f_path}: {e}\")\n","\n","    if not datasets:\n","        raise RuntimeError(\"No dataset found!\")\n","\n","    print(f\"Loaded {len(datasets)} chunks.\")\n","    return ConcatDataset(datasets)"],"metadata":{"id":"DJGvbNPZfDTK","executionInfo":{"status":"ok","timestamp":1764257708709,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hồ Quang Chung","userId":"12751878129593085762"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["!pip install torch torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YMBy0bijqbMr","executionInfo":{"status":"ok","timestamp":1764257713221,"user_tz":-420,"elapsed":4514,"user":{"displayName":"Hồ Quang Chung","userId":"12751878129593085762"}},"outputId":"eb356fcb-c21c-4af2-9d22-736d042ce687"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"]}]},{"cell_type":"code","source":["import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","# --- CẤU HÌNH ---\n","DATA_DIR = '/content/Datasets/processed' # Thư mục đã giải nén ở Cell 2\n","BATCH_SIZE = 256              # T4 GPU chịu được 128-256 mẫu\n","EPOCHS = 10\n","LEARNING_RATE = 0.001\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def train():\n","    print(f\"Training on device: {DEVICE}\")\n","\n","    dataset = get_dataset(DATA_DIR)\n","    # num_workers=2 để load dữ liệu song song\n","    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","\n","    model = GoResNet(num_blocks=10, num_filters=128).to(DEVICE)\n","\n","    criterion_policy = nn.CrossEntropyLoss()\n","    criterion_value = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","    for epoch in range(EPOCHS):\n","        model.train()\n","        total_loss = 0\n","\n","        print(f\"Start Epoch {epoch+1}...\")\n","\n","        for batch_idx, batch in enumerate(dataloader):\n","            features = batch['feature'].to(DEVICE)\n","            target_policy = batch['policy_target'].to(DEVICE)\n","            target_value = batch['value_target'].to(DEVICE)\n","\n","            optimizer.zero_grad()\n","            pred_policy, pred_value = model(features)\n","\n","            loss_p = criterion_policy(pred_policy, target_policy)\n","            loss_v = criterion_value(pred_value.squeeze(), target_value)\n","            loss = loss_p + loss_v\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","            if batch_idx % 50 == 0:\n","                print(f\"  Batch {batch_idx} | Loss: {loss.item():.4f}\")\n","\n","        avg_loss = total_loss / len(dataloader)\n","        print(f\"Epoch {epoch+1} Done. Avg Loss: {avg_loss:.4f}\")\n","\n","        # Lưu model vào Drive để không bị mất khi tắt Colab\n","        save_path = f\"/content/go_model_epoch_{epoch+1}.pth\"\n","        torch.save(model.state_dict(), save_path)\n","        print(f\"Saved checkpoint to Drive: {save_path}\")\n","\n","# Gọi hàm train\n","train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6g6TzXpQqiOS","outputId":"37ec8753-a5f1-4fd1-ad4c-35314e467625"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on device: cuda\n","Scanning chunks in /content/Datasets/processed...\n","Loaded 15 chunks.\n","Start Epoch 1...\n","  Batch 0 | Loss: 7.0288\n","  Batch 50 | Loss: 6.7111\n","  Batch 100 | Loss: 6.7103\n","  Batch 150 | Loss: 6.0325\n","  Batch 200 | Loss: 5.6108\n","  Batch 250 | Loss: 5.3915\n","  Batch 300 | Loss: 5.4478\n","  Batch 350 | Loss: 5.1579\n","  Batch 400 | Loss: 4.9802\n","  Batch 450 | Loss: 5.0842\n","  Batch 500 | Loss: 4.8453\n","  Batch 550 | Loss: 5.0354\n","  Batch 600 | Loss: 4.5062\n","  Batch 650 | Loss: 4.4162\n","  Batch 700 | Loss: 4.1753\n","  Batch 750 | Loss: 4.1625\n","  Batch 800 | Loss: 3.8793\n","  Batch 850 | Loss: 3.7574\n","  Batch 900 | Loss: 3.8575\n","  Batch 950 | Loss: 3.9408\n","  Batch 1000 | Loss: 3.4699\n","  Batch 1050 | Loss: 3.7500\n","  Batch 1100 | Loss: 3.5532\n","  Batch 1150 | Loss: 3.3970\n","  Batch 1200 | Loss: 3.4445\n","  Batch 1250 | Loss: 3.3177\n","  Batch 1300 | Loss: 3.3861\n","  Batch 1350 | Loss: 3.4579\n","  Batch 1400 | Loss: 3.3104\n","  Batch 1450 | Loss: 3.4620\n","  Batch 1500 | Loss: 3.5746\n","  Batch 1550 | Loss: 3.3582\n","  Batch 1600 | Loss: 3.3390\n","  Batch 1650 | Loss: 3.1536\n","  Batch 1700 | Loss: 3.3145\n","  Batch 1750 | Loss: 3.1569\n","  Batch 1800 | Loss: 3.1433\n","  Batch 1850 | Loss: 3.0546\n","  Batch 1900 | Loss: 3.2516\n","  Batch 1950 | Loss: 3.1073\n","  Batch 2000 | Loss: 3.2502\n","  Batch 2050 | Loss: 2.8813\n","  Batch 2100 | Loss: 2.9826\n","  Batch 2150 | Loss: 3.1401\n","  Batch 2200 | Loss: 3.2064\n","  Batch 2250 | Loss: 2.9904\n","  Batch 2300 | Loss: 3.3074\n","  Batch 2350 | Loss: 3.0143\n","  Batch 2400 | Loss: 2.9039\n","  Batch 2450 | Loss: 3.0824\n","  Batch 2500 | Loss: 3.1087\n","  Batch 2550 | Loss: 2.9056\n","  Batch 2600 | Loss: 3.1096\n","  Batch 2650 | Loss: 2.7421\n","  Batch 2700 | Loss: 2.8588\n","  Batch 2750 | Loss: 2.8091\n","Epoch 1 Done. Avg Loss: 3.8183\n","Saved checkpoint to Drive: /content/go_model_epoch_1.pth\n","Start Epoch 2...\n","  Batch 0 | Loss: 2.6998\n","  Batch 50 | Loss: 2.5951\n","  Batch 100 | Loss: 2.9120\n","  Batch 150 | Loss: 2.7964\n","  Batch 200 | Loss: 2.7813\n","  Batch 250 | Loss: 2.7972\n","  Batch 300 | Loss: 2.9300\n","  Batch 350 | Loss: 2.8999\n","  Batch 400 | Loss: 2.4589\n","  Batch 450 | Loss: 2.8294\n","  Batch 500 | Loss: 2.5750\n","  Batch 550 | Loss: 2.6905\n","  Batch 600 | Loss: 2.8162\n","  Batch 650 | Loss: 2.7095\n","  Batch 700 | Loss: 2.7779\n","  Batch 750 | Loss: 2.7436\n","  Batch 800 | Loss: 2.7401\n","  Batch 850 | Loss: 2.9000\n","  Batch 900 | Loss: 2.6781\n","  Batch 950 | Loss: 2.8438\n","  Batch 1000 | Loss: 2.8240\n","  Batch 1050 | Loss: 2.6475\n","  Batch 1100 | Loss: 2.8424\n","  Batch 1150 | Loss: 2.7699\n","  Batch 1200 | Loss: 2.5767\n","  Batch 1250 | Loss: 2.7478\n","  Batch 1300 | Loss: 2.4903\n","  Batch 1350 | Loss: 2.7967\n","  Batch 1400 | Loss: 2.8264\n","  Batch 1450 | Loss: 2.6501\n","  Batch 1500 | Loss: 2.7647\n","  Batch 1550 | Loss: 2.6571\n","  Batch 1600 | Loss: 2.7064\n","  Batch 1650 | Loss: 2.6393\n","  Batch 1700 | Loss: 2.7462\n","  Batch 1750 | Loss: 2.4834\n","  Batch 1800 | Loss: 2.6154\n","  Batch 1850 | Loss: 2.6178\n","  Batch 1900 | Loss: 2.8613\n","  Batch 1950 | Loss: 2.8707\n","  Batch 2000 | Loss: 2.3540\n","  Batch 2050 | Loss: 2.6749\n","  Batch 2100 | Loss: 2.7062\n","  Batch 2150 | Loss: 2.7167\n","  Batch 2200 | Loss: 2.5617\n","  Batch 2250 | Loss: 2.5917\n","  Batch 2300 | Loss: 2.5733\n","  Batch 2350 | Loss: 2.7020\n","  Batch 2400 | Loss: 2.8931\n","  Batch 2450 | Loss: 2.8438\n","  Batch 2500 | Loss: 2.8135\n","  Batch 2550 | Loss: 2.5907\n","  Batch 2600 | Loss: 2.6512\n","  Batch 2650 | Loss: 2.6115\n","  Batch 2700 | Loss: 2.5959\n","  Batch 2750 | Loss: 2.7073\n","Epoch 2 Done. Avg Loss: 2.7303\n","Saved checkpoint to Drive: /content/go_model_epoch_2.pth\n","Start Epoch 3...\n","  Batch 0 | Loss: 2.6033\n","  Batch 50 | Loss: 2.3920\n","  Batch 100 | Loss: 2.2315\n","  Batch 150 | Loss: 2.5444\n","  Batch 200 | Loss: 2.6774\n","  Batch 250 | Loss: 2.4816\n","  Batch 300 | Loss: 2.5434\n","  Batch 350 | Loss: 2.4743\n","  Batch 400 | Loss: 2.4425\n","  Batch 450 | Loss: 2.3972\n","  Batch 500 | Loss: 2.5309\n","  Batch 550 | Loss: 2.5454\n","  Batch 600 | Loss: 2.7075\n","  Batch 650 | Loss: 2.6583\n","  Batch 700 | Loss: 2.4181\n","  Batch 750 | Loss: 2.4669\n","  Batch 800 | Loss: 2.4203\n","  Batch 850 | Loss: 2.7748\n","  Batch 900 | Loss: 2.4832\n","  Batch 950 | Loss: 2.3681\n","  Batch 1000 | Loss: 2.7223\n","  Batch 1050 | Loss: 2.3966\n","  Batch 1100 | Loss: 2.5597\n","  Batch 1150 | Loss: 2.3506\n","  Batch 1200 | Loss: 2.2847\n","  Batch 1250 | Loss: 2.4946\n","  Batch 1300 | Loss: 2.6771\n","  Batch 1350 | Loss: 2.2900\n","  Batch 1400 | Loss: 2.3947\n","  Batch 1450 | Loss: 2.2935\n","  Batch 1500 | Loss: 2.5574\n","  Batch 1550 | Loss: 2.6916\n","  Batch 1600 | Loss: 2.3462\n","  Batch 1650 | Loss: 2.5943\n","  Batch 1700 | Loss: 2.3914\n","  Batch 1750 | Loss: 2.4408\n","  Batch 1800 | Loss: 2.7280\n","  Batch 1850 | Loss: 2.4944\n","  Batch 1900 | Loss: 2.6427\n","  Batch 1950 | Loss: 2.5474\n","  Batch 2000 | Loss: 2.1872\n","  Batch 2050 | Loss: 2.3733\n","  Batch 2100 | Loss: 2.5417\n","  Batch 2150 | Loss: 2.6178\n","  Batch 2200 | Loss: 2.5499\n","  Batch 2250 | Loss: 2.4990\n","  Batch 2300 | Loss: 2.4831\n","  Batch 2350 | Loss: 2.2089\n","  Batch 2400 | Loss: 2.3908\n","  Batch 2450 | Loss: 2.3827\n","  Batch 2500 | Loss: 2.5727\n","  Batch 2550 | Loss: 2.3652\n","  Batch 2600 | Loss: 2.5183\n","  Batch 2650 | Loss: 2.3227\n","  Batch 2700 | Loss: 2.7376\n","  Batch 2750 | Loss: 2.5584\n","Epoch 3 Done. Avg Loss: 2.4838\n","Saved checkpoint to Drive: /content/go_model_epoch_3.pth\n","Start Epoch 4...\n","  Batch 0 | Loss: 2.1390\n","  Batch 50 | Loss: 2.3105\n","  Batch 100 | Loss: 2.2239\n","  Batch 150 | Loss: 2.2533\n","  Batch 200 | Loss: 2.2066\n","  Batch 250 | Loss: 2.2577\n","  Batch 300 | Loss: 2.2321\n","  Batch 350 | Loss: 2.0796\n","  Batch 400 | Loss: 2.3404\n","  Batch 450 | Loss: 2.3051\n","  Batch 500 | Loss: 2.3874\n","  Batch 550 | Loss: 2.4017\n","  Batch 600 | Loss: 2.3612\n","  Batch 650 | Loss: 2.1554\n","  Batch 700 | Loss: 2.2926\n","  Batch 750 | Loss: 2.3388\n","  Batch 800 | Loss: 2.2840\n","  Batch 850 | Loss: 2.5294\n","  Batch 900 | Loss: 2.3535\n","  Batch 950 | Loss: 2.3020\n","  Batch 1000 | Loss: 2.4410\n","  Batch 1050 | Loss: 2.2461\n","  Batch 1100 | Loss: 2.1790\n","  Batch 1150 | Loss: 2.6066\n","  Batch 1200 | Loss: 2.2572\n","  Batch 1250 | Loss: 2.3514\n","  Batch 1300 | Loss: 2.2906\n","  Batch 1350 | Loss: 2.3660\n","  Batch 1400 | Loss: 2.2458\n","  Batch 1450 | Loss: 2.1063\n","  Batch 1500 | Loss: 2.5233\n","  Batch 1550 | Loss: 2.3460\n","  Batch 1600 | Loss: 2.2909\n","  Batch 1650 | Loss: 2.2529\n","  Batch 1700 | Loss: 2.4504\n","  Batch 1750 | Loss: 2.3319\n","  Batch 1800 | Loss: 2.3150\n","  Batch 1850 | Loss: 2.3260\n","  Batch 1900 | Loss: 2.3912\n","  Batch 1950 | Loss: 2.1786\n","  Batch 2000 | Loss: 2.6091\n","  Batch 2050 | Loss: 2.4628\n","  Batch 2100 | Loss: 2.2821\n","  Batch 2150 | Loss: 2.2406\n","  Batch 2200 | Loss: 2.3483\n","  Batch 2250 | Loss: 2.3739\n","  Batch 2300 | Loss: 2.3883\n","  Batch 2350 | Loss: 2.2844\n","  Batch 2400 | Loss: 2.4410\n","  Batch 2450 | Loss: 2.4577\n","  Batch 2500 | Loss: 2.6895\n","  Batch 2550 | Loss: 2.3593\n","  Batch 2600 | Loss: 2.3336\n","  Batch 2650 | Loss: 2.5691\n","  Batch 2700 | Loss: 2.3662\n","  Batch 2750 | Loss: 2.2421\n","Epoch 4 Done. Avg Loss: 2.3280\n","Saved checkpoint to Drive: /content/go_model_epoch_4.pth\n","Start Epoch 5...\n","  Batch 0 | Loss: 2.2322\n","  Batch 50 | Loss: 2.1946\n","  Batch 100 | Loss: 2.1436\n","  Batch 150 | Loss: 2.0280\n","  Batch 200 | Loss: 2.0366\n","  Batch 250 | Loss: 2.1147\n","  Batch 300 | Loss: 2.2364\n","  Batch 350 | Loss: 2.2185\n","  Batch 400 | Loss: 2.1612\n","  Batch 450 | Loss: 2.2830\n","  Batch 500 | Loss: 2.1334\n","  Batch 550 | Loss: 1.9722\n","  Batch 600 | Loss: 2.1222\n","  Batch 650 | Loss: 2.2875\n","  Batch 700 | Loss: 2.2551\n","  Batch 750 | Loss: 2.2004\n","  Batch 800 | Loss: 2.1790\n","  Batch 850 | Loss: 2.2477\n","  Batch 900 | Loss: 2.2374\n","  Batch 950 | Loss: 2.3054\n","  Batch 1000 | Loss: 2.2004\n","  Batch 1050 | Loss: 2.3188\n","  Batch 1100 | Loss: 2.2951\n","  Batch 1150 | Loss: 1.9360\n","  Batch 1200 | Loss: 2.0566\n","  Batch 1250 | Loss: 2.3022\n","  Batch 1300 | Loss: 2.1659\n","  Batch 1350 | Loss: 2.2410\n","  Batch 1400 | Loss: 2.2371\n","  Batch 1450 | Loss: 2.2579\n","  Batch 1500 | Loss: 2.3312\n","  Batch 1550 | Loss: 2.4758\n","  Batch 1600 | Loss: 2.1905\n","  Batch 1650 | Loss: 2.3234\n","  Batch 1700 | Loss: 2.1040\n","  Batch 1750 | Loss: 2.2225\n","  Batch 1800 | Loss: 2.2909\n","  Batch 1850 | Loss: 2.3222\n","  Batch 1900 | Loss: 2.2323\n","  Batch 1950 | Loss: 2.2628\n","  Batch 2000 | Loss: 2.3104\n","  Batch 2050 | Loss: 2.3837\n","  Batch 2100 | Loss: 2.2983\n","  Batch 2150 | Loss: 2.2109\n","  Batch 2200 | Loss: 2.2780\n","  Batch 2250 | Loss: 2.2044\n","  Batch 2300 | Loss: 2.2700\n","  Batch 2350 | Loss: 1.9702\n","  Batch 2400 | Loss: 2.1550\n","  Batch 2450 | Loss: 2.0744\n","  Batch 2500 | Loss: 2.3503\n","  Batch 2550 | Loss: 2.0867\n","  Batch 2600 | Loss: 2.1449\n","  Batch 2650 | Loss: 2.4242\n","  Batch 2700 | Loss: 2.2230\n","  Batch 2750 | Loss: 2.3073\n","Epoch 5 Done. Avg Loss: 2.2120\n","Saved checkpoint to Drive: /content/go_model_epoch_5.pth\n","Start Epoch 6...\n","  Batch 0 | Loss: 2.0459\n","  Batch 50 | Loss: 2.0061\n","  Batch 100 | Loss: 2.0142\n","  Batch 150 | Loss: 2.2535\n","  Batch 200 | Loss: 2.1421\n","  Batch 250 | Loss: 1.8987\n","  Batch 300 | Loss: 1.9032\n","  Batch 350 | Loss: 2.0156\n","  Batch 400 | Loss: 1.9803\n","  Batch 450 | Loss: 1.8932\n","  Batch 500 | Loss: 1.9018\n","  Batch 550 | Loss: 1.9712\n","  Batch 600 | Loss: 2.1143\n","  Batch 650 | Loss: 2.2028\n","  Batch 700 | Loss: 2.1207\n","  Batch 750 | Loss: 2.0313\n","  Batch 800 | Loss: 1.8171\n","  Batch 850 | Loss: 2.0026\n","  Batch 900 | Loss: 2.1773\n","  Batch 950 | Loss: 2.1164\n","  Batch 1000 | Loss: 2.1911\n","  Batch 1050 | Loss: 2.0464\n","  Batch 1100 | Loss: 2.4040\n","  Batch 1150 | Loss: 2.1854\n","  Batch 1200 | Loss: 2.0880\n","  Batch 1250 | Loss: 2.1347\n","  Batch 1300 | Loss: 2.2451\n","  Batch 1350 | Loss: 2.0377\n","  Batch 1400 | Loss: 2.2111\n","  Batch 1450 | Loss: 2.1613\n","  Batch 1500 | Loss: 2.2177\n","  Batch 1550 | Loss: 2.1210\n","  Batch 1600 | Loss: 2.1549\n","  Batch 1650 | Loss: 2.0584\n","  Batch 1700 | Loss: 2.1487\n","  Batch 1750 | Loss: 2.3031\n","  Batch 1800 | Loss: 2.3101\n","  Batch 1850 | Loss: 2.1494\n","  Batch 1900 | Loss: 2.0402\n","  Batch 1950 | Loss: 2.1071\n","  Batch 2000 | Loss: 2.1063\n","  Batch 2050 | Loss: 2.1418\n","  Batch 2100 | Loss: 2.0201\n","  Batch 2150 | Loss: 2.1794\n","  Batch 2200 | Loss: 2.2396\n","  Batch 2250 | Loss: 2.0998\n","  Batch 2300 | Loss: 1.9918\n","  Batch 2350 | Loss: 2.1376\n","  Batch 2400 | Loss: 1.9741\n","  Batch 2450 | Loss: 1.9317\n","  Batch 2500 | Loss: 2.2209\n","  Batch 2550 | Loss: 2.0098\n","  Batch 2600 | Loss: 2.0700\n","  Batch 2650 | Loss: 2.3486\n","  Batch 2700 | Loss: 2.0575\n","  Batch 2750 | Loss: 2.2274\n","Epoch 6 Done. Avg Loss: 2.1138\n","Saved checkpoint to Drive: /content/go_model_epoch_6.pth\n","Start Epoch 7...\n","  Batch 0 | Loss: 1.8748\n","  Batch 50 | Loss: 1.8599\n","  Batch 100 | Loss: 1.8676\n","  Batch 150 | Loss: 1.8878\n","  Batch 200 | Loss: 1.8242\n","  Batch 250 | Loss: 1.8730\n","  Batch 300 | Loss: 1.9392\n","  Batch 350 | Loss: 2.0043\n","  Batch 400 | Loss: 1.8834\n","  Batch 450 | Loss: 2.0341\n","  Batch 500 | Loss: 2.0132\n","  Batch 550 | Loss: 1.9411\n","  Batch 600 | Loss: 2.0412\n","  Batch 650 | Loss: 2.0344\n","  Batch 700 | Loss: 1.9450\n","  Batch 750 | Loss: 2.0108\n","  Batch 800 | Loss: 1.8657\n","  Batch 850 | Loss: 1.7703\n","  Batch 900 | Loss: 2.0482\n","  Batch 950 | Loss: 2.0339\n","  Batch 1000 | Loss: 2.1458\n","  Batch 1050 | Loss: 2.0744\n","  Batch 1100 | Loss: 1.9164\n","  Batch 1150 | Loss: 2.0631\n","  Batch 1200 | Loss: 1.8533\n","  Batch 1250 | Loss: 2.2402\n","  Batch 1300 | Loss: 2.0382\n","  Batch 1350 | Loss: 2.0156\n","  Batch 1400 | Loss: 2.0149\n","  Batch 1450 | Loss: 1.9122\n","  Batch 1500 | Loss: 2.0704\n","  Batch 1550 | Loss: 2.0451\n","  Batch 1600 | Loss: 2.0840\n","  Batch 1650 | Loss: 2.0715\n","  Batch 1700 | Loss: 1.9703\n","  Batch 1750 | Loss: 1.9786\n","  Batch 1800 | Loss: 1.9216\n","  Batch 1850 | Loss: 2.3384\n","  Batch 1900 | Loss: 2.1410\n","  Batch 1950 | Loss: 2.0383\n","  Batch 2000 | Loss: 2.1089\n","  Batch 2050 | Loss: 2.1121\n","  Batch 2100 | Loss: 1.9102\n","  Batch 2150 | Loss: 2.1221\n","  Batch 2200 | Loss: 2.1679\n","  Batch 2250 | Loss: 2.0458\n","  Batch 2300 | Loss: 2.1226\n","  Batch 2350 | Loss: 2.0790\n","  Batch 2400 | Loss: 1.9948\n","  Batch 2450 | Loss: 2.2539\n","  Batch 2500 | Loss: 2.2180\n","  Batch 2550 | Loss: 2.0775\n","  Batch 2600 | Loss: 2.0988\n","  Batch 2650 | Loss: 2.0297\n","  Batch 2700 | Loss: 2.1765\n","  Batch 2750 | Loss: 1.9392\n","Epoch 7 Done. Avg Loss: 2.0268\n","Saved checkpoint to Drive: /content/go_model_epoch_7.pth\n","Start Epoch 8...\n","  Batch 0 | Loss: 1.9248\n","  Batch 50 | Loss: 1.8105\n","  Batch 100 | Loss: 1.9119\n","  Batch 150 | Loss: 1.7483\n","  Batch 200 | Loss: 1.9153\n","  Batch 250 | Loss: 1.8193\n","  Batch 300 | Loss: 1.9237\n","  Batch 350 | Loss: 1.8561\n","  Batch 400 | Loss: 1.8783\n","  Batch 450 | Loss: 1.8903\n","  Batch 500 | Loss: 1.9009\n","  Batch 550 | Loss: 2.0154\n","  Batch 600 | Loss: 1.9815\n","  Batch 650 | Loss: 1.7677\n","  Batch 700 | Loss: 1.8052\n","  Batch 750 | Loss: 1.9849\n","  Batch 800 | Loss: 2.1717\n","  Batch 850 | Loss: 1.9340\n","  Batch 900 | Loss: 2.0303\n","  Batch 950 | Loss: 1.7920\n","  Batch 1000 | Loss: 2.0321\n","  Batch 1050 | Loss: 2.1022\n","  Batch 1100 | Loss: 1.9666\n","  Batch 1150 | Loss: 2.0221\n","  Batch 1200 | Loss: 2.0961\n","  Batch 1250 | Loss: 2.1536\n","  Batch 1300 | Loss: 1.9860\n","  Batch 1350 | Loss: 1.9422\n","  Batch 1400 | Loss: 2.0793\n","  Batch 1450 | Loss: 2.0813\n","  Batch 1500 | Loss: 1.8157\n","  Batch 1550 | Loss: 2.0849\n","  Batch 1600 | Loss: 1.9766\n","  Batch 1650 | Loss: 1.7881\n","  Batch 1700 | Loss: 2.1236\n","  Batch 1750 | Loss: 2.2244\n","  Batch 1800 | Loss: 1.7335\n","  Batch 1850 | Loss: 2.0871\n","  Batch 1900 | Loss: 1.9139\n","  Batch 1950 | Loss: 2.1986\n","  Batch 2000 | Loss: 1.9604\n","  Batch 2050 | Loss: 1.9510\n","  Batch 2100 | Loss: 2.1392\n","  Batch 2150 | Loss: 1.9162\n","  Batch 2200 | Loss: 2.1226\n","  Batch 2250 | Loss: 2.1711\n","  Batch 2300 | Loss: 1.8002\n","  Batch 2350 | Loss: 2.0187\n","  Batch 2400 | Loss: 1.7206\n","  Batch 2450 | Loss: 2.0580\n","  Batch 2500 | Loss: 2.0967\n","  Batch 2550 | Loss: 2.0478\n","  Batch 2600 | Loss: 2.0053\n","  Batch 2650 | Loss: 1.9107\n","  Batch 2700 | Loss: 2.0262\n","  Batch 2750 | Loss: 1.9797\n","Epoch 8 Done. Avg Loss: 1.9445\n","Saved checkpoint to Drive: /content/go_model_epoch_8.pth\n","Start Epoch 9...\n","  Batch 0 | Loss: 1.9665\n","  Batch 50 | Loss: 1.9880\n","  Batch 100 | Loss: 1.5941\n","  Batch 150 | Loss: 1.5877\n","  Batch 200 | Loss: 1.7378\n","  Batch 250 | Loss: 1.6804\n","  Batch 300 | Loss: 1.8188\n","  Batch 350 | Loss: 1.7511\n","  Batch 400 | Loss: 1.7868\n","  Batch 450 | Loss: 1.9242\n","  Batch 500 | Loss: 1.8007\n","  Batch 550 | Loss: 1.8339\n","  Batch 600 | Loss: 1.8040\n","  Batch 650 | Loss: 1.7092\n","  Batch 700 | Loss: 1.7617\n","  Batch 750 | Loss: 1.9521\n","  Batch 800 | Loss: 1.8890\n","  Batch 850 | Loss: 1.7712\n","  Batch 900 | Loss: 1.8083\n","  Batch 950 | Loss: 1.9302\n","  Batch 1000 | Loss: 1.8722\n","  Batch 1050 | Loss: 1.9802\n","  Batch 1100 | Loss: 2.1155\n","  Batch 1150 | Loss: 1.8255\n","  Batch 1200 | Loss: 1.7524\n","  Batch 1250 | Loss: 1.8397\n","  Batch 1300 | Loss: 1.7970\n","  Batch 1350 | Loss: 1.9885\n","  Batch 1400 | Loss: 2.0549\n","  Batch 1450 | Loss: 2.0398\n","  Batch 1500 | Loss: 1.9112\n","  Batch 1550 | Loss: 1.8732\n","  Batch 1600 | Loss: 1.8851\n","  Batch 1650 | Loss: 1.8994\n","  Batch 1700 | Loss: 1.7469\n","  Batch 1750 | Loss: 1.8892\n","  Batch 1800 | Loss: 1.7127\n","  Batch 1850 | Loss: 1.8382\n","  Batch 1900 | Loss: 1.8639\n","  Batch 1950 | Loss: 1.8930\n","  Batch 2000 | Loss: 1.7114\n","  Batch 2050 | Loss: 1.7378\n","  Batch 2100 | Loss: 1.9727\n","  Batch 2150 | Loss: 1.9820\n","  Batch 2200 | Loss: 1.8449\n","  Batch 2250 | Loss: 2.0070\n","  Batch 2300 | Loss: 2.1439\n","  Batch 2350 | Loss: 1.9388\n","  Batch 2400 | Loss: 1.9503\n","  Batch 2450 | Loss: 2.1437\n","  Batch 2500 | Loss: 1.9798\n","  Batch 2550 | Loss: 1.9353\n","  Batch 2600 | Loss: 1.9370\n","  Batch 2650 | Loss: 1.8217\n","  Batch 2700 | Loss: 1.8430\n","  Batch 2750 | Loss: 1.8936\n","Epoch 9 Done. Avg Loss: 1.8649\n","Saved checkpoint to Drive: /content/go_model_epoch_9.pth\n","Start Epoch 10...\n","  Batch 0 | Loss: 1.5598\n","  Batch 50 | Loss: 1.6405\n","  Batch 100 | Loss: 1.6630\n","  Batch 150 | Loss: 1.5520\n","  Batch 200 | Loss: 1.6195\n","  Batch 250 | Loss: 1.7094\n","  Batch 300 | Loss: 1.5728\n","  Batch 350 | Loss: 1.7968\n","  Batch 400 | Loss: 1.7969\n","  Batch 450 | Loss: 1.8979\n","  Batch 500 | Loss: 1.6330\n","  Batch 550 | Loss: 1.7532\n","  Batch 600 | Loss: 1.8485\n","  Batch 650 | Loss: 1.8685\n","  Batch 700 | Loss: 1.5653\n","  Batch 750 | Loss: 1.6479\n","  Batch 800 | Loss: 1.6203\n","  Batch 850 | Loss: 1.7298\n","  Batch 900 | Loss: 1.6923\n","  Batch 950 | Loss: 1.6632\n","  Batch 1000 | Loss: 1.8042\n","  Batch 1050 | Loss: 1.8324\n","  Batch 1100 | Loss: 1.8857\n","  Batch 1150 | Loss: 1.6228\n","  Batch 1200 | Loss: 1.8504\n","  Batch 1250 | Loss: 1.9209\n","  Batch 1300 | Loss: 1.7834\n","  Batch 1350 | Loss: 1.6424\n","  Batch 1400 | Loss: 1.7981\n","  Batch 1450 | Loss: 1.7581\n","  Batch 1500 | Loss: 1.8792\n","  Batch 1550 | Loss: 1.6664\n","  Batch 1600 | Loss: 1.9313\n","  Batch 1650 | Loss: 1.7988\n","  Batch 1700 | Loss: 1.9782\n","  Batch 1750 | Loss: 1.9027\n","  Batch 1800 | Loss: 1.6769\n","  Batch 1850 | Loss: 1.8728\n","  Batch 1900 | Loss: 1.7081\n","  Batch 1950 | Loss: 1.8652\n","  Batch 2000 | Loss: 1.8093\n","  Batch 2050 | Loss: 1.9837\n","  Batch 2100 | Loss: 1.8714\n","  Batch 2150 | Loss: 1.6660\n","  Batch 2200 | Loss: 1.8701\n","  Batch 2250 | Loss: 1.9828\n","  Batch 2300 | Loss: 1.9687\n","  Batch 2350 | Loss: 1.9254\n","  Batch 2400 | Loss: 1.9340\n","  Batch 2450 | Loss: 1.9233\n","  Batch 2500 | Loss: 1.6159\n","  Batch 2550 | Loss: 1.8697\n","  Batch 2600 | Loss: 1.7445\n","  Batch 2650 | Loss: 1.6727\n","  Batch 2700 | Loss: 1.6444\n"]}]},{"cell_type":"markdown","source":["## FOR FINETUNE ONLY"],"metadata":{"id":"Srb0Qu2yrUd0"}},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","# --- CẤU HÌNH FINE-TUNE ---\n","PRETRAINED_PATH = \"/content/go_model_epoch_10.pth\" # Model cũ\n","NEW_DATA_DIR = \"/content/Datasets\" # Folder chứa dataset mới (dạng .npy)\n","BATCH_SIZE = 256\n","EPOCHS = 5               # Fine-tune thường cần ít epoch hơn train mới\n","LEARNING_RATE = 0.0005   # <--- QUAN TRỌNG: Giảm nhỏ hơn lúc train gốc (thường là 1/10)\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def fine_tune():\n","    print(f\"Fine-tuning on device: {DEVICE}\")\n","\n","    # 1. Khởi tạo kiến trúc mạng\n","    # LƯU Ý: num_blocks và num_filters PHẢI GIỐNG HỆT lúc train model cũ\n","    model = GoResNet(num_blocks=10, num_filters=128).to(DEVICE)\n","\n","    # 2. Load trọng số cũ (Weights)\n","    print(f\"Loading weights from {PRETRAINED_PATH}...\")\n","    try:\n","        # map_location để đảm bảo load được dù train trên GPU khác hay CPU\n","        state_dict = torch.load(PRETRAINED_PATH, map_location=DEVICE)\n","        model.load_state_dict(state_dict)\n","        print(\"Weights loaded successfully!\")\n","    except Exception as e:\n","        print(f\"Error loading weights: {e}\")\n","        return\n","\n","    # 3. Chuẩn bị dữ liệu mới\n","    # Dùng hàm get_dataset cũ nhưng trỏ vào folder dữ liệu mới\n","    dataset = get_dataset(NEW_DATA_DIR)\n","    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","\n","    # 4. Optimizer với Learning Rate thấp\n","    criterion_policy = nn.CrossEntropyLoss()\n","    criterion_value = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) # LR nhỏ\n","\n","    # 5. Vòng lặp Training (như cũ)\n","    for epoch in range(EPOCHS):\n","        model.train()\n","        total_loss = 0\n","\n","        print(f\"Start Fine-tuning Epoch {epoch+1}...\")\n","\n","        for batch_idx, batch in enumerate(dataloader):\n","            features = batch['feature'].to(DEVICE)\n","            target_policy = batch['policy_target'].to(DEVICE)\n","            target_value = batch['value_target'].to(DEVICE)\n","\n","            optimizer.zero_grad()\n","            pred_policy, pred_value = model(features)\n","\n","            loss_p = criterion_policy(pred_policy, target_policy)\n","            loss_v = criterion_value(pred_value.squeeze(), target_value)\n","            loss = loss_p + loss_v\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","            if batch_idx % 100 == 0:\n","                print(f\"  Batch {batch_idx} | Loss: {loss.item():.4f}\")\n","\n","        # Lưu model fine-tune với tên mới\n","        save_path = f\"/content/drive/MyDrive/GoAI/finetuned_epoch_{epoch+1}.pth\"\n","        torch.save(model.state_dict(), save_path)\n","        print(f\"Saved finetuned model: {save_path}\")\n","\n","if __name__ == \"__main__\":\n","    fine_tune()"],"metadata":{"id":"18GIVMAorYCt"},"execution_count":null,"outputs":[]}]}